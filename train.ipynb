{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up a simple training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt; plt.style.use('dark_background')\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 100\n",
    "# quantization parameters\n",
    "EPSI = 0.15 # quantization step\n",
    "MAX_SIG = 2.5 # maximum value of the signal\n",
    "NLEVELS = int(2*MAX_SIG / EPSI)//2+1 # number of quantization levels\n",
    "print(f\"Quantization step: {EPSI}, Number of levels: {NLEVELS}\")\n",
    "# training parameters\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LR = 3e-3\n",
    "LATENT_DIM = 16\n",
    "N_HIDDEN = 128\n",
    "# dataset parameters\n",
    "N_DATASET = 20_000\n",
    "N_FREQS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim=128):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "    def forward(self, x): return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset of random signals\n",
    "class SigDS(Dataset):\n",
    "    def __init__(self, n_ds):\n",
    "        self.n_ds = n_ds\n",
    "        self.data = th.stack([create_random_signal(N_SAMPLES, N_FREQS) for _ in range(n_ds)])\n",
    "    def __len__(self): return self.n_ds\n",
    "    def __getitem__(self, idx): return self.data[idx]\n",
    "\n",
    "ds = SigDS(N_DATASET)\n",
    "dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 10 random signals\n",
    "plt.figure(figsize=(10, 3))\n",
    "for i in range(10):\n",
    "    plt.subplot(5, 2, i+1)\n",
    "    plt.plot(ds[i])\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "plt.tight_layout(), plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "enc = Net(N_SAMPLES, LATENT_DIM, N_HIDDEN)\n",
    "dec = Net(LATENT_DIM, N_SAMPLES, N_HIDDEN)\n",
    "opt = optim.Adam(list(enc.parameters()) + list(dec.parameters()), lr=LR)\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "l1_loss = nn.L1Loss()\n",
    "# H_loss = HLoss1()\n",
    "# H_loss = HLoss2(EPSI, MAX_SIG)\n",
    "H_loss = HLoss3(EPSI, MAX_SIG, temperature=0.1)\n",
    "\n",
    "lmses, lL1s, lHs = [], [], []\n",
    "for epoch in (range(N_EPOCHS)):\n",
    "    elmse, elL1, elH = 0, 0, 0\n",
    "    for x in dl:\n",
    "        opt.zero_grad()\n",
    "\n",
    "        z = enc(x)\n",
    "        x̂ = dec(z)\n",
    "\n",
    "        lmse = mse_loss(x̂, x)\n",
    "        lL1 = l1_loss(x̂, x)\n",
    "        lH = H_loss(x̂, x)\n",
    "\n",
    "        elmse += lmse.item()\n",
    "        elL1 += lL1.item()\n",
    "        elH += lH.item()\n",
    "\n",
    "        # loss = lmse\n",
    "        # loss = lL1\n",
    "        # loss = lH \n",
    "        loss = lH + 0.1*lmse\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "    lmses.append(elmse/len(dl))\n",
    "    lL1s.append(elL1/len(dl))\n",
    "    lHs.append(elH/len(dl))\n",
    "\n",
    "    print(f'ep {epoch} -> mse:{lmses[-1]:.4f}, L1:{lL1s[-1]:.4f}, H:{lHs[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "lmses, lL1s, lHs =  th.tensor(lmses), th.tensor(lL1s), th.tensor(lHs)\n",
    "lmses = lmses/th.max(lmses)\n",
    "lL1s = lL1s/th.max(lL1s)\n",
    "lHs = lHs/th.max(lHs)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(3,1, 1)\n",
    "plt.plot(lmses, label='mse')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.subplot(3,1, 2)\n",
    "plt.plot(lL1s, label='L1')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.subplot(3,1, 3)\n",
    "plt.plot(lHs, label='H')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'train losses: mse:{lmses[-1]:.4f}, L1:{lL1s[-1]:.4f}, H:{lHs[-1]:.4f}')\n",
    "\n",
    "\n",
    "# evaluate on unseen data useing mse loss\n",
    "test_in = SigDS(100).data\n",
    "test_out = dec(enc(SigDS(100).data))\n",
    "test_loss = mse_loss(test_out, test_in)\n",
    "print(f'test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare input and output\n",
    "plt.figure(figsize=(10, 10))\n",
    "NPLOTS = 40\n",
    "err = th.zeros(NPLOTS, 3)\n",
    "for i in range(NPLOTS):\n",
    "    plt.subplot(10, 4, i+1)\n",
    "    x = create_random_signal(N_SAMPLES).view(1, N_SAMPLES)\n",
    "    x̂ = dec(enc(x))\n",
    "    err[i] = th.tensor([mse_loss(x̂, x), l1_loss(x̂, x), H_loss(x̂, x)])\n",
    "    x, x̂ = x.view(-1).detach(), x̂.view(-1).detach()\n",
    "    plt.plot(x, label='input')\n",
    "    plt.plot(x̂, label='output')\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "plt.suptitle(f'mse:{err[:,0].mean():.4f},  L1:{err[:,1].mean():.4f},  H:{err[:,2].mean():.4f}')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
