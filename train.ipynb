{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up a simple training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt; plt.style.use('dark_background')\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 100\n",
    "# quantization parameters\n",
    "EPSI = 0.15 # quantization step\n",
    "MAX_SIG = 2.5 # maximum value of the signal\n",
    "NLEVELS = int(2*MAX_SIG / EPSI)//2+1 # number of quantization levels\n",
    "print(f\"Quantization step: {EPSI}, Number of levels: {NLEVELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture\n",
    "LATENT_DIM = 8\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim=64):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "    def forward(self, x): return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset of random signals\n",
    "class SigDS(Dataset):\n",
    "    def __init__(self, n_ds):\n",
    "        self.n_ds = n_ds\n",
    "        self.data = th.stack([create_random_signal(N_SAMPLES) for _ in range(n_ds)])\n",
    "    def __len__(self): return self.n_ds\n",
    "    def __getitem__(self, idx): return self.data[idx]\n",
    "\n",
    "ds = SigDS(20000)\n",
    "dl = DataLoader(ds, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 10 random signals\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(10):\n",
    "    plt.subplot(5, 2, i+1)\n",
    "    plt.plot(ds[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom loss with custom gradient\n",
    "class HLoss1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HLoss1, self).__init__()\n",
    "    def forward(self, x1, x2):\n",
    "        x = x1 - x2\n",
    "        b = F.softmax(x, dim=-1) * F.log_softmax(x, dim=-1)\n",
    "        # b = -1.0 * b.sum() / x.size(0)\n",
    "        b = b.sum() / x.size(0)\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "enc = Net(N_SAMPLES, LATENT_DIM)\n",
    "dec = Net(LATENT_DIM, N_SAMPLES)\n",
    "opt = optim.Adam(list(enc.parameters()) + list(dec.parameters()), lr=3e-4)\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "l1_loss = nn.L1Loss()\n",
    "# H_loss = HLoss1()\n",
    "H_loss = HLoss2(EPSI, MAX_SIG)\n",
    "\n",
    "n_epochs = 5\n",
    "lmses, lL1s, lHs = [], [], []\n",
    "for epoch in (range(n_epochs)):\n",
    "    elmse, elL1, elH = 0, 0, 0\n",
    "    for x in dl:\n",
    "        opt.zero_grad()\n",
    "\n",
    "        z = enc(x)\n",
    "        x̂ = dec(z)\n",
    "\n",
    "        lmse = mse_loss(x̂, x)\n",
    "        lL1 = l1_loss(x̂, x)\n",
    "        lH = H_loss(x̂, x)\n",
    "\n",
    "        elmse += lmse.item()\n",
    "        elL1 += lL1.item()\n",
    "        elH += lH.item()\n",
    "\n",
    "        # loss = lmse\n",
    "        # loss = lL1\n",
    "        loss = lH\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "    lmses.append(elmse/len(dl))\n",
    "    lL1s.append(elL1/len(dl))\n",
    "    lHs.append(elH/len(dl))\n",
    "\n",
    "    print(f'ep {epoch}-> mse:{lmses[-1]:.4f}, L1:{lL1s[-1]:.4f}, H:{lHs[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "lmses, lL1s, lHs =  th.tensor(lmses), th.tensor(lL1s), th.tensor(lHs)\n",
    "lmses = lmses/th.max(lmses)\n",
    "lL1s = lL1s/th.max(lL1s)\n",
    "lHs = lHs/th.max(lHs)\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.plot(lmses, label='mse')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.plot(lL1s, label='L1')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.plot(lHs, label='H')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'train losses: mse:{lmses[-1]:.4f}, L1:{lL1s[-1]:.4f}, H:{lHs[-1]:.4f}')\n",
    "\n",
    "\n",
    "# evaluate on unseen data useing mse loss\n",
    "test_in = SigDS(100).data\n",
    "test_out = dec(enc(SigDS(100).data))\n",
    "test_loss = mse_loss(test_out, test_in)\n",
    "print(f'test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare input and output\n",
    "plt.figure(figsize=(10, 10))\n",
    "err = []\n",
    "for i in range(20):\n",
    "    plt.subplot(10, 2, i+1)\n",
    "    x = create_random_signal(N_SAMPLES)\n",
    "    x̂ = dec(enc(th.tensor(x).view(1,N_SAMPLES))).view(N_SAMPLES).detach().numpy()\n",
    "    err.append(th.mean((x-x̂)**2))\n",
    "    plt.plot(x, label='input')\n",
    "    plt.plot(x̂, label='output')\n",
    "    # plt.grid()\n",
    "plt.suptitle(f'mse: {th.mean(th.tensor(err))}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
