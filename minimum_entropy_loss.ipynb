{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understaning Minimum Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization parameters\n",
    "EPSI = 0.5 # quantization step\n",
    "MAX_SIG = 2.5 # maximum value of the signal (symmetric around 0)\n",
    "NLEVELS = 2*int(MAX_SIG/EPSI) + 1 # number of quantization levels\n",
    "LEVELS = th.tensor([i*EPSI for i in range(-int(MAX_SIG/EPSI), 1)] + [i*EPSI for i in range(1, int(MAX_SIG/EPSI)+1)])\n",
    "print(f\"Quantization step: {EPSI}, Number of levels: {NLEVELS}, \\nLEVELS: {LEVELS}\")\n",
    "assert len(LEVELS) == NLEVELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random signal as a sum of random frequencies\n",
    "N_FREQS = 5\n",
    "N_SAMPLES = 100\n",
    "x = create_random_signal(N_SAMPLES, N_FREQS)\n",
    "\n",
    "#plot the signal\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.stem(x)\n",
    "plt.title('Signal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantize the signal\n",
    "xq = quantize(x, LEVELS)\n",
    "\n",
    "print(f'levels: {th.unique(xq)}')\n",
    "print(f'number of levels: {th.unique(xq).shape[0]}')\n",
    "print(f'calc number of levels: {NLEVELS}')\n",
    "\n",
    "# plot the quantized signal\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.stem(xq, label='quantized')\n",
    "plt.title('Quantized Signal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft quantize the signal\n",
    "temp = 1\n",
    "xsq, xsa = soft_quantize(x, LEVELS, temp)\n",
    "\n",
    "# plot the quantized signal\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.stem(xq, label='quantized')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.title('Soft Quantized Signal')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(xsa.T, aspect='auto', origin='lower', interpolation='none')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.title('Soft Assignment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Entropy\n",
    "### in different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard entropy, caculated counting the number of times each level appears\n",
    "h1 = entropy(xq)\n",
    "print(f'Standard entropy: {h1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENTROPY IS NOT DIFFERENTIABLE\n",
    "## But apparently these mutherfuckers found a way to do it\n",
    "$$\n",
    " \\frac{\\partial{H}}{\\partial{r_i}} = \\lim_{b \\to \\infty} \\sum_{j=0}^{|S|} [1 + \\ln p(s_j)] * R(r_i - s_j)\n",
    "$$\n",
    "\n",
    "with $R$:\n",
    "\n",
    "$$\n",
    "R(r_i - s_j) = \\frac{b}{|r|\\varepsilon^b} \\frac{(r_i - s_j)^{b-1}}{\\left[\\frac{(r_i -\n",
    "s_j)^b}{\\varepsilon^b} + 1\\right]^2} $$\n",
    "\n",
    "Master thesis version:\n",
    "\n",
    "$$ \n",
    "R = \\frac{b}{\\left( \\text{size}(rq) \\cdot \\varepsilon^b \\right)} \\cdot \\frac{(rq - s_j)^{b-1}}{\\left( \\frac{(rq - s_j)^b}{\\varepsilon^b} + 1 \\right)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # see what this fucking function actually looks like\n",
    "# import numpy as np\n",
    "# def dentropy(rq, b=10.0, ε=0.1):\n",
    "#     symbols, counts = np.unique(rq, return_counts=True)\n",
    "#     p = counts/len(rq)\n",
    "#     # logp = np.log2(p + 1e-8)\n",
    "#     logp = np.log(p + 1e-8)\n",
    "#     H = -np.sum(p*logp) # entropy\n",
    "#     sizer = len(rq)\n",
    "#     DH = 0\n",
    "#     for j in range(len(symbols)):\n",
    "#         DH += (1+logp[j])*b / (sizer*ε**b) * (rq-symbols[j])**(b-1) / (((rq-symbols[j])/ε)**b+1)**2\n",
    "#     return H, DH\n",
    "\n",
    "# H, DH = dentropy(xq, b=10, ε=EPSI)\n",
    "\n",
    "# print(f'Entropy: {H:.2f}')\n",
    "# print(f'Gradient: {DH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dentropy2(rq, ε=0.1): # importance sampling based entropy calculation #https://en.wikipedia.org/wiki/Kernel_density_estimation\n",
    "#     def normal(x, μ, σ): return np.exp(-0.5*((x-μ)/σ)**2)/(σ*np.sqrt(2*π))\n",
    "    \n",
    "#     # sample m points from a isotropic gaussian\n",
    "#     m = 300\n",
    "#     samples = np.random.randn(m)\n",
    "#     # samples = np.linspace(-1, 1, m)\n",
    "#     likelihoods = normal(samples, 0, 1)\n",
    "\n",
    "#     σ = 5*ε\n",
    "\n",
    "#     #calculate pdf of the quantized signal\n",
    "#     tot = 0\n",
    "#     for s,l in zip(samples, likelihoods):\n",
    "#         p = np.mean(normal(s, rq, σ))\n",
    "#         ent = -p*np.log(p+1e-8) / l\n",
    "#         tot += ent\n",
    "#     entropy = tot/m \n",
    "\n",
    "#     return entropy\n",
    "\n",
    "# H = dentropy2(xq, ε=EPSI)\n",
    "# print(f'Entropy: {H:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see if there is correlation between the differentiable functions and the real entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on a lot of tries\n",
    "Hx, Hy = [], []\n",
    "h1_loss = HLoss1(LEVELS) \n",
    "h2_loss = HLoss2(LEVELS) \n",
    "# h3_loss = HLoss3(LEVELS) \n",
    "for _ in range(2000):\n",
    "    # generate a random signal as a sum of random frequencies\n",
    "    x = create_random_signal(N_SAMPLES, N_FREQS)\n",
    "\n",
    "    # X\n",
    "    # measure entropy of the quantizedsignal\n",
    "    Hx.append(entropy(quantize(x, LEVELS))) \n",
    "    # Hx.append(h2_loss(x.view(1,-1)).item())\n",
    "\n",
    "    # Y\n",
    "    # Hy.append(h1_loss(x.view(1,-1)).item())\n",
    "    Hy.append(h2_loss(x.view(1,-1)).item())\n",
    "    # Hy.append(h3_loss(x.view(1,-1)).item())\n",
    "\n",
    "Hx, Hy = th.tensor(Hx), th.tensor(Hy)\n",
    "\n",
    "# get the best linear fit between Hx and Hy\n",
    "A = th.vstack([Hx, th.ones(len(Hx))]).T\n",
    "m, c = th.linalg.lstsq(A, Hy, rcond=None)[0]\n",
    "print(f'best fit: y = {m:.2f}x + {c:.2f}')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(Hx, Hy, s=1)\n",
    "plt.plot(Hx, m*Hx + c, color='red')\n",
    "plt.xlabel('Entropy 1')\n",
    "plt.ylabel('Entropy 2')\n",
    "# plt.ylim([0, 8])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
