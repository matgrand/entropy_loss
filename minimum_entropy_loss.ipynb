{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum Entropy Loss\n",
    "https://discuss.pytorch.org/t/calculating-the-entropy-loss/14510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import matplotlib.pyplot as plt; plt.style.use('dark_background')\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization parameters\n",
    "EPSI = 0.15 # quantization step\n",
    "MAX_SIG = 2.5 # maximum value of the signal\n",
    "NLEVELS = int(2*MAX_SIG / EPSI)//2+1 # number of quantization levels\n",
    "print(f\"Quantization step: {EPSI}, Number of levels: {NLEVELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random signal as a sum of random frequencies\n",
    "N_FREQS = 5\n",
    "N_SAMPLES = 100\n",
    "x = create_random_signal(N_SAMPLES, N_FREQS)\n",
    "\n",
    "#plot the signal\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.stem(x)\n",
    "plt.title('Signal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantize the signal\n",
    "xq = quantize(x, EPSI, MAX_SIG)\n",
    "\n",
    "print(f'levels: {th.unique(xq)}')\n",
    "print(f'number of levels: {th.unique(xq).shape[0]}')\n",
    "print(f'calc number of levels: {NLEVELS}')\n",
    "\n",
    "# plot the quantized signal\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.stem(xq, label='quantized')\n",
    "plt.title('Quantized Signal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to a vector of 1-hot encoded values\n",
    "xq_tmp = th.round(xq/(2*EPSI)).long() + NLEVELS//2\n",
    "print(f'xq_tmp: {th.unique(xq_tmp)}')\n",
    "xq_1hot = F.one_hot(xq_tmp, num_classes=NLEVELS).float()\n",
    "# plot the 2dmatrix \n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.imshow(xq_1hot.T, aspect='auto', interpolation='none', origin='lower')\n",
    "plt.title('One-hot encoded signal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Entropy\n",
    "### in different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard entropy, caculated counting the number of times each level appears\n",
    "h1 = entropy(xq)\n",
    "print(f'Standard entropy: {h1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate entropy using the softmax bs\n",
    "\n",
    "hloss1 = HLoss1(EPSI, MAX_SIG)\n",
    "h = hloss1(x)\n",
    "\n",
    "print(f'Entropy: {h.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENTROPY IS NOT DIFFERENTIABLE\n",
    "## But apparently these mutherfuckers found a way to do it\n",
    "$$\n",
    " \\frac{\\partial{H}}{\\partial{r_i}} = \\lim_{b \\to \\infty} \\sum_{j=0}^{|S|} [1 + \\ln p(s_j)] * R(r_i - s_j)\n",
    "$$\n",
    "\n",
    "with $R$:\n",
    "\n",
    "$$\n",
    "R(r_i - s_j) = \\frac{b}{|r|\\varepsilon^b} \\frac{(r_i - s_j)^{b-1}}{\\left[\\frac{(r_i -\n",
    "s_j)^b}{\\varepsilon^b} + 1\\right]^2} $$\n",
    "\n",
    "Master thesis version:\n",
    "\n",
    "$$ \n",
    "R = \\frac{b}{\\left( \\text{size}(rq) \\cdot \\varepsilon^b \\right)} \\cdot \\frac{(rq - s_j)^{b-1}}{\\left( \\frac{(rq - s_j)^b}{\\varepsilon^b} + 1 \\right)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # see what this fucking function actually looks like\n",
    "# import numpy as np\n",
    "# def dentropy(rq, b=10.0, ε=0.1):\n",
    "#     symbols, counts = np.unique(rq, return_counts=True)\n",
    "#     p = counts/len(rq)\n",
    "#     # logp = np.log2(p + 1e-8)\n",
    "#     logp = np.log(p + 1e-8)\n",
    "#     H = -np.sum(p*logp) # entropy\n",
    "#     sizer = len(rq)\n",
    "#     DH = 0\n",
    "#     for j in range(len(symbols)):\n",
    "#         DH += (1+logp[j])*b / (sizer*ε**b) * (rq-symbols[j])**(b-1) / (((rq-symbols[j])/ε)**b+1)**2\n",
    "#     return H, DH\n",
    "\n",
    "# H, DH = dentropy(xq, b=10, ε=EPSI)\n",
    "\n",
    "# print(f'Entropy: {H:.2f}')\n",
    "# print(f'Gradient: {DH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dentropy2(rq, ε=0.1): # importance sampling based entropy calculation #https://en.wikipedia.org/wiki/Kernel_density_estimation\n",
    "#     def normal(x, μ, σ): return np.exp(-0.5*((x-μ)/σ)**2)/(σ*np.sqrt(2*π))\n",
    "    \n",
    "#     # sample m points from a isotropic gaussian\n",
    "#     m = 300\n",
    "#     samples = np.random.randn(m)\n",
    "#     # samples = np.linspace(-1, 1, m)\n",
    "#     likelihoods = normal(samples, 0, 1)\n",
    "\n",
    "#     σ = 5*ε\n",
    "\n",
    "#     #calculate pdf of the quantized signal\n",
    "#     tot = 0\n",
    "#     for s,l in zip(samples, likelihoods):\n",
    "#         p = np.mean(normal(s, rq, σ))\n",
    "#         ent = -p*np.log(p+1e-8) / l\n",
    "#         tot += ent\n",
    "#     entropy = tot/m \n",
    "\n",
    "#     return entropy\n",
    "\n",
    "# H = dentropy2(xq, ε=EPSI)\n",
    "# print(f'Entropy: {H:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_pt(rq, ε=0.1): #https://en.wikipedia.org/wiki/Kernel_density_estimation\n",
    "    def normal(x, μ, σ): return th.exp(-0.5*((x-μ)/σ)**2)/(σ*math.sqrt(2*π))\n",
    "\n",
    "    # rq = rq-th.mean(rq)\n",
    "    σ = 5*ε # width of the gaussian kernel\n",
    "    # sample m points from a isotropic gaussian\n",
    "    m = 300\n",
    "    samples = th.randn(m)\n",
    "    likelihoods = normal(samples, 0, 1)\n",
    "    #calculate pdf of the quantized signal\n",
    "    ent = 0\n",
    "    for s,l in zip(samples, likelihoods):\n",
    "        p = th.mean(normal(s, rq, σ))\n",
    "        ent += -p*th.log(p+1e-8) / l\n",
    "    return ent/m\n",
    "\n",
    "# #create a nn module from the entropy function\n",
    "# class HLoss2(nn.Module):\n",
    "#     def __init__(self, ε=0.1):\n",
    "#         super(HLoss2, self).__init__()\n",
    "#         self.ε = ε\n",
    "    \n",
    "#     def normal(self, x, μ, σ): return th.exp(-0.5*((x-μ)/σ)**2)/(σ*np.sqrt(2*π))\n",
    "#     def forward(self, x1, x2):\n",
    "#         r = x1 - x2\n",
    "        \n",
    "#         σ = 5*self.ε # width of the gaussian kernel\n",
    "#         # sample m points from a isotropic gaussian\n",
    "#         m = 300\n",
    "#         samples = th.randn(m)\n",
    "#         likelihoods = self.normal(samples, 0, 1)\n",
    "#         #calculate pdf of the quantized signal\n",
    "#         ent = 0\n",
    "#         for s,l in zip(samples, likelihoods):\n",
    "#             p = th.mean(self.normal(s, rq, σ))\n",
    "#             ent += -p*th.log(p+1e-8) / l\n",
    "#         return ent/m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see if there is correlation between the softmax differentiable function and the real entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on a lot of tries\n",
    "import numpy as np\n",
    "H1s, H2s = [], []\n",
    "h1_loss = HLoss1(EPSI, MAX_SIG)\n",
    "h2_loss = HLoss2(EPSI, MAX_SIG)\n",
    "for _ in tqdm(range(2000)):\n",
    "    # generate a random signal as a sum of random frequencies\n",
    "    x = create_random_signal(N_SAMPLES, N_FREQS)\n",
    "\n",
    "    # quantize the signal\n",
    "    xqi = quantize(x, EPSI, MAX_SIG)\n",
    "\n",
    "    # measure entropy of the signal\n",
    "    H1s.append(entropy(xqi)) \n",
    "\n",
    "    # # H2\n",
    "    # H2s.append(h1_loss(x).item())\n",
    "    \n",
    "    H2s.append(h2_loss(x).item())\n",
    "\n",
    "    \n",
    "\n",
    "H1s, H2s = np.array(H1s), np.array(H2s)\n",
    "\n",
    "# get the best linear fit between H1s and H2s\n",
    "A = np.vstack([H1s, np.ones(len(H1s))]).T\n",
    "m, c = np.linalg.lstsq(A, H2s, rcond=None)[0]\n",
    "print(f'best fit: y = {m:.2f}x + {c:.2f}')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(H1s, H2s, s=5)\n",
    "plt.plot(H1s, m*H1s + c, color='red')\n",
    "plt.xlabel('Entropy 1')\n",
    "plt.ylabel('Entropy 2')\n",
    "# plt.ylim([0, 8])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = HLoss()\n",
    "# x = Variable(th.randn(10, 10))\n",
    "# w = Variable(th.randn(10, 3), requires_grad=True)\n",
    "# output = th.matmul(x, w)\n",
    "# loss = criterion(output)\n",
    "# loss.backward()\n",
    "# print(w.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
