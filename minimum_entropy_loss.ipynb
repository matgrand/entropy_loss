{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum Entropy Loss\n",
    "https://discuss.pytorch.org/t/calculating-the-entropy-loss/14510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.style.use('dark_background')\n",
    "π = np.pi\n",
    "from numpy.random import uniform as uni\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random signal as a sum of random frequencies\n",
    "N_FREQS = 5\n",
    "N_SAMPLES = 100\n",
    "\n",
    "# generate random frequencies\n",
    "fs = uni(0, N_SAMPLES/50, N_FREQS)\n",
    "As = uni(0, 1, N_FREQS)\n",
    "φs = uni(0, 2*π, N_FREQS)\n",
    "t = np.arange(N_SAMPLES)\n",
    "\n",
    "# generate the signal\n",
    "x = np.sum([As[i]*np.sin(2*π*fs[i]*t+φs[i]) for i in range(N_FREQS)], axis=0)\n",
    "# x = uni(-uni(.2, 1.2), +uni(.2, 1.2), N_SAMPLES)\n",
    "\n",
    "#plot the signal\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.scatter(t,x, s=1)\n",
    "plt.title('Signal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantize the signal\n",
    "EPSI = 0.1\n",
    "\n",
    "nlevels = 1+np.ceil(np.max(np.abs(x))/EPSI)\n",
    "levels = EPSI*np.arange(-nlevels, nlevels+1)\n",
    "nlevels = int(1+2*np.ceil(np.max(np.abs(x))/EPSI))\n",
    "print(f'levels: {levels}')\n",
    "print(f'number of levels: {nlevels}')\n",
    "\n",
    "# def quantize(x, ε): return ε*np.round(x/ε) \n",
    "def quantize(x, ε): return 2*ε*np.round(x/(2*ε)) \n",
    "\n",
    "xq = quantize(x, EPSI)\n",
    "\n",
    "print(f'levels: {np.unique(xq)}')\n",
    "print(f'number of levels: {np.unique(xq).shape[0]}')\n",
    "\n",
    "# plot the quantized signal\n",
    "plt.figure(figsize=(10, 2))\n",
    "# plt.plot(t,x, label='original')\n",
    "plt.scatter(t,xq, label='quantized', s=1)\n",
    "plt.title('Quantized Signal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure entropy of the signal\n",
    "def entropy(x):\n",
    "    _, counts = np.unique(x, return_counts=True)\n",
    "    p = counts/len(x)\n",
    "    # return -np.sum(p*np.log2(p))\n",
    "    return -np.sum(p*np.log(p))\n",
    "\n",
    "print(f'Entropy of the quantized signal: {entropy(xq):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HLoss, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.size(-1) == nlevels, f'x.size: {x.size()}'\n",
    "        b = F.softmax(x, dim=-1) * F.log_softmax(x, dim=-1)\n",
    "        b = -1.0 * b.sum()\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the signal to a tensor\n",
    "xqt = torch.tensor(xq/EPSI + nlevels//2, dtype=torch.int64)\n",
    "# #convert to a vector of 1-hot encoded values\n",
    "xq_1hot = F.one_hot(xqt, num_classes=nlevels)\n",
    "# plot the 2dmatrix \n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.imshow(xq_1hot.T, aspect='auto', interpolation='none', origin='lower')\n",
    "plt.title('Quantized Signal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate entropy\n",
    "hloss = HLoss()\n",
    "h = hloss(xq_1hot.to(torch.float32).unsqueeze(0))/N_SAMPLES\n",
    "print(f'Entropy of the quantized signal: {h:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENTROPY IS NOT DIFFERENTIABLE\n",
    "## But apparently these mutherfuckers found a way to do it\n",
    "$$\n",
    " \\frac{\\partial{H}}{\\partial{r_i}} = \\lim_{b \\to \\infty} \\sum_{j=0}^{|S|} [1 + \\ln p(s_j)] * R(r_i - s_j)\n",
    "$$\n",
    "\n",
    "with $R$:\n",
    "\n",
    "$$\n",
    "R(r_i - s_j) = \\frac{b}{|r|\\varepsilon^b} \\frac{(r_i - s_j)^{b-1}}{\\left[\\frac{(r_i -\n",
    "s_j)^b}{\\varepsilon^b} + 1\\right]^2} $$\n",
    "\n",
    "Master thesis version:\n",
    "\n",
    "$$ \n",
    "R = \\frac{b}{\\left( \\text{size}(rq) \\cdot \\varepsilon^b \\right)} \\cdot \\frac{(rq - s_j)^{b-1}}{\\left( \\frac{(rq - s_j)^b}{\\varepsilon^b} + 1 \\right)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what this fucking function actually looks like\n",
    "def dentropy(rq, b=10.0, ε=0.1):\n",
    "    symbols, counts = np.unique(rq, return_counts=True)\n",
    "    p = counts/len(rq)\n",
    "    # logp = np.log2(p + 1e-8)\n",
    "    logp = np.log(p + 1e-8)\n",
    "    H = -np.sum(p*logp) # entropy\n",
    "    sizer = len(rq)\n",
    "    DH = 0\n",
    "    for j in range(len(symbols)):\n",
    "        DH += (1+logp[j])*b / (sizer*ε**b) * (rq-symbols[j])**(b-1) / (((rq-symbols[j])/ε)**b+1)**2\n",
    "    return H, DH\n",
    "\n",
    "H, DH = dentropy(xq, b=10, ε=EPSI)\n",
    "\n",
    "print(f'Entropy: {H:.2f}')\n",
    "print(f'Gradient: {DH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dentropy2(rq, ε=0.1): # importance sampling based entropy calculation #https://en.wikipedia.org/wiki/Kernel_density_estimation\n",
    "    def normal(x, μ, σ): return np.exp(-0.5*((x-μ)/σ)**2)/(σ*np.sqrt(2*π))\n",
    "    \n",
    "    # sample m points from a isotropic gaussian\n",
    "    m = 300\n",
    "    samples = np.random.randn(m)\n",
    "    # samples = np.linspace(-1, 1, m)\n",
    "    likelihoods = normal(samples, 0, 1)\n",
    "\n",
    "    σ = 5*ε\n",
    "\n",
    "    #calculate pdf of the quantized signal\n",
    "    tot = 0\n",
    "    for s,l in zip(samples, likelihoods):\n",
    "        p = np.mean(normal(s, rq, σ))\n",
    "        ent = -p*np.log(p+1e-8) / l\n",
    "        tot += ent\n",
    "    entropy = tot/m \n",
    "\n",
    "    return entropy\n",
    "\n",
    "H = dentropy2(xq, ε=EPSI)\n",
    "print(f'Entropy: {H:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_pt(rq, ε=0.1): #https://en.wikipedia.org/wiki/Kernel_density_estimation\n",
    "    def normal(x, μ, σ): return torch.exp(-0.5*((x-μ)/σ)**2)/(σ*np.sqrt(2*π))\n",
    "\n",
    "    # rq = rq-torch.mean(rq)\n",
    "    σ = 5*ε # width of the gaussian kernel\n",
    "    # sample m points from a isotropic gaussian\n",
    "    m = 300\n",
    "    samples = torch.randn(m)\n",
    "    likelihoods = normal(samples, 0, 1)\n",
    "    #calculate pdf of the quantized signal\n",
    "    ent = 0\n",
    "    for s,l in zip(samples, likelihoods):\n",
    "        p = torch.mean(normal(s, rq, σ))\n",
    "        ent += -p*torch.log(p+1e-8) / l\n",
    "    return ent/m\n",
    "\n",
    "#create a nn module from the entropy function\n",
    "class HLoss2(nn.Module):\n",
    "    def __init__(self, ε=0.1):\n",
    "        super(HLoss2, self).__init__()\n",
    "        self.ε = ε\n",
    "    \n",
    "    def normal(self, x, μ, σ): return torch.exp(-0.5*((x-μ)/σ)**2)/(σ*np.sqrt(2*π))\n",
    "    def forward(self, x1, x2):\n",
    "        r = x1 - x2\n",
    "        \n",
    "        σ = 5*self.ε # width of the gaussian kernel\n",
    "        # sample m points from a isotropic gaussian\n",
    "        m = 300\n",
    "        samples = torch.randn(m)\n",
    "        likelihoods = self.normal(samples, 0, 1)\n",
    "        #calculate pdf of the quantized signal\n",
    "        ent = 0\n",
    "        for s,l in zip(samples, likelihoods):\n",
    "            p = torch.mean(self.normal(s, rq, σ))\n",
    "            ent += -p*torch.log(p+1e-8) / l\n",
    "        return ent/m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see if there is correlation between the softmax differentiable function and the real entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on a lot of tries\n",
    "H1s, H2s = [], []\n",
    "for _ in tqdm(range(2000)):\n",
    "    # generate random frequencies\n",
    "    fs = uni(0, N_SAMPLES/50, N_FREQS)\n",
    "    As = uni(0, 1, N_FREQS)\n",
    "    φs = uni(0, 2*π, N_FREQS)\n",
    "    t = np.arange(N_SAMPLES)\n",
    "\n",
    "    # generate the signal\n",
    "    x = np.sum([As[i]*np.sin(2*π*fs[i]*t+φs[i]) for i in range(N_FREQS)], axis=0)\n",
    "\n",
    "    # quantize the signal\n",
    "    xqi = quantize(x, EPSI)\n",
    "\n",
    "    nlevels = int(1+2*np.ceil(np.max(np.abs(x))/EPSI))\n",
    "\n",
    "    # measure entropy of the signal\n",
    "    H1s.append(entropy(xqi)) \n",
    "\n",
    "    # # H2\n",
    "    # # convert the signal to a tensor\n",
    "    # xqit = torch.tensor(xqi/EPSI + nlevels//2, dtype=torch.int64)\n",
    "    # #convert to a vector of 1-hot encoded values\n",
    "    # xqi_1hot = F.one_hot(xqit, num_classes=nlevels)\n",
    "    # # calculate entropy 2 \n",
    "    # h = hloss(xqi_1hot.to(torch.float32).unsqueeze(0))/N_SAMPLES\n",
    "    # H2s.append(h)\n",
    "    \n",
    "    xqi_pt = torch.tensor(xqi)\n",
    "    # xqi_pt = torch.tensor(x)\n",
    "    H2s.append(entropy_pt(xqi_pt, ε=EPSI).item())\n",
    "\n",
    "    \n",
    "\n",
    "H1s, H2s = np.array(H1s), np.array(H2s)\n",
    "\n",
    "# get the best linear fit between H1s and H2s\n",
    "A = np.vstack([H1s, np.ones(len(H1s))]).T\n",
    "m, c = np.linalg.lstsq(A, H2s, rcond=None)[0]\n",
    "print(f'best fit: y = {m:.2f}x + {c:.2f}')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(H1s, H2s, s=5)\n",
    "plt.plot(H1s, m*H1s + c, color='red')\n",
    "plt.xlabel('Entropy 1')\n",
    "plt.ylabel('Entropy 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = HLoss()\n",
    "# x = Variable(torch.randn(10, 10))\n",
    "# w = Variable(torch.randn(10, 3), requires_grad=True)\n",
    "# output = torch.matmul(x, w)\n",
    "# loss = criterion(output)\n",
    "# loss.backward()\n",
    "# print(w.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
